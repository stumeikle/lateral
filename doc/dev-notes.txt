20161119

A note about sequences.
Its normal enough to use db sequences to assign ids when writing to the db.
Ids are not present until the object is persisted. Fine. We'd annotate entities with lines like:

@GeneratedValue(strategy=GenerationType.AUTO)
@SequenceGenerator(name="my_seq",sequenceName="MY_SEQ", allocationSize=1)

We'd expect with Lateral, when write-through is enabled, that the usual handling of sequences would
occur.

This is not the case. Alas we need to pass new entities to our cache layer _before_ they are
write-through en to the db, and in the case of hazelcast at least, we can't pass in an object
with no sequence. So the sequence aspects are tied to the choice of cache.

I'll set things up to allow objects with null ids to be passed through to the cache layer, in
case another cache is used which allows the write-through to happen before the id is populated
and the object stored in the case.

For the hazelcast case i'll need to enable the plugin or generator to be configured to support
sequencing on particular fields. this has some other consequences -- hc ids are longs but we may
expect or specify other types in our proto. the domain is being affected by the choice of
cache layer. :(
This is very interesting in relation also: https://github.com/hazelcast/hazelcast/issues/11
it really is much better to just use guids
[NEW FEATURE - support generated int ids]
  [info: generators might go into default factory impl. need to set start value tho]

[BUG] [Done]
if I set @RepositoryId on a primitive field in my proto then compilation fails. but it shouldn't.
The generator needs to be fixed to swap types if the user specifies @RepositoryId on a primitive.

[BUG] [Done]

or feature. if i call create through the REST API i do need to get the created objec id back.
even if its generated by a cache layer sequence

[FEATURE, ISSUE]

did I ever check the dbdumpers function properly or at all in failover mode?

[BUG] [done]

HC repository impls have parmeter names with capitals


20161119 Sequences again

.. are hard. Whether write-through or write-behind, if the system is restarted the next sequence id needs to
come from the db. this means the whole system needs to wait on the db processes being up first. in the case
of write-behind, the db might not even be up to date. it could be that the queue is still being written to the
db.

so the cache layer can't be initialised until the db is up.hmm

in the case of write-behind we were going to have a system which injected the current set of objects from the db
into the caches. (oi like). this process could also inject the sequence initiation values. how does that work in
the write-through case? and how would the other components know to wait on the oi process?

for HC. the map store class on the server attempts to load all keys on startup. we could hook into that.
we might need a concept of 'com'. if h/c goes down (chance=?) and the dbdumper is an hour behind we won't want
to wait an hour to restart. so we'd need a way to correctly repopulate the cache. how to do that.
current HC is tied into that throught he mapstore. maybe we should use a file store to back up the cache for the
write behind case. though thats not ideal.

Next job:
sort out the admin bus.
then use it to only allow access to the cache when the cache has been correctly populated at startup
do this in both write-behind and write-through cases
then use these mechanisms to support generated sequences
and then finally ensure that clients get their id values back when creating

No access to cache if cache not initialised
no writing to cache if dbdumper process not up (or equivalent with write-through)
so we use the maps and the admin repository
we don't want the admin cache to be persisted, btw
if the users have configured no persistence for the cache then just proceed as usual
consider write-through with multiple servers. even if we use the map store to load the cache the secondardy
client needs to know to wait before they use the cache. maybe hazelcast does that for us. this is cache by cache
loading also, which might be nice
maybe we just hook into the mapstore then

20161121

mapstore ok but for write behind, mapstore would need to send admin command to db dumpers to populate the cache.
and that's a whole mechansim missing. ie command send, command receive, do we say only 1 db dumper should handle?
how do we lock, how do we ensure only one responds, what if that system crashes before the cache loaded, etc

20161122

Think we'll do... send command to admin bus. use optimistic locking to allow only 1 dbd to receive command.
dbd goes and does its stuff. if it fails for any reason, use manual means to resend the admin command.
once its done it can set the cache loaded flag. Q: how do we get events back to the clients? a broadcast event
would be good. can entities watch for changes on a given map entry in an adhoc manner? not very easily it looks
like ... http://docs.hazelcast.org/docs/3.5/manual/html/map-maplistener.html
might as well poll as create such a structure. though maybe everything could listen for events on the admin bus

[BUGs, GAPs x 1000000] db dumper functionality is very sketchy. no file buffer. unclear if active-pass supported yet

[MISSING FEATURE ... client apps should be able to select which of the distributed caches that they connect to
there's no need for them to connect to them all]

20161127

Getting on with it.
(Loading of the caches on startup)
All good. Now we have a slight change of things though. previously we only had mapstores
for the write-through case. Now we'll have map stores in all cases, and use that in the
write-behind case to manage the loading of the caches (sending commands to the remote db
dumpers). that's fine. in the write behind case though we'll need to skip the other
methods. So, mapstore functionality differs between write-though and write-behind quite
a bit more substantially

20161128

Single system. Write through. Ideally synchronous writes. Non synchronous writes rely on hz cache being up else
data loss will occur. Additionally using mapstore to perform the writes means updates to object may be missed and
can't be guaranteed to be persisted. User should be able to configure load from db and initial load from db

Write behind. Currently dbd listens to the bus to ensure that all updates can be audited and all changes persisted to
db. I think thats fine. We don't need additional mapstore communications overhead if we have a busy system. we know
the changes already. it allows us to split dbds by cache and so on too. but then we need to send a command to populate
the caches initially and to decide if the user wants individual loads from db too. we need to go via dbd because the
updates may not have been persisted to db yet and only dbd has enough info to know the current state
in this case load, loadAllKeys, will go to dbd and store will do nothing (in the mapstore). its a pity we can't split
the map store using the hc api. maybe we can. maploader. cooooool. loooks like we can either have a loader or a load
and store. but no store on its own.

user options: pick loader, storer, write their own
            : pick local or remote

if we have a plugin for loading a plugin for storing and plugins for remote connections that should cover it
then the mapstorefactory will just provide proxies to the plugins
mapstorefactory anyway is injected.

so. if write-through or read-through then install the mapstorefactory.
this proxies to the plugins
then plugin selection will change the behaviour from local to remote etc
might need to review if we need the persister and retriever classes after that
we could simply configure the mapstore factory implementation in the config and then
say if there -is- a config then install the mapstore factory

eg
        //di.class.for.HCMapStoreFactory=transgenic.lauterbrunnen.lateral.persist.hazelcast.generated.HCReadThroughFactoryImpl
        //di.class.for.HCMapStoreFactory=transgenic.lauterbrunnen.lateral.persist.hazelcast.generated.HCWriteThroughFactoryImpl
        //di.class.for.HCMapStoreFactory=transgenic.lauterbrunnen.lateral.persist.hazelcast.generated.HCReadWriteThroughFactoryImpl

the pluginfactories might need to be generated of course
hmm.
bit more twisty turny
maybe the plugins can specify the implementation

so. getting there.
hc embedded plugin checks for read-through or write-through config and if present installs our factory
the factory will use the config to then choose the read and/or write map store imps
and the read and/or write imps will then inject the persister and retriever

The map store factory can configure the injections for the write-through and read-through
although probably needs to defer to the config if there is any

Task for now: update the admin bus to allow returning results of commands

20161129

craeteing the remote retriever. next = create direct and remote persister [done]
update entities to have jpql for retrieving keys [done]
create the RT WT proxies [done]
fix the retrievers to convert repositoryids [done]

fix HCCacheChangeManagerImpl [done]

** Get to point where micro can save entries via rest, retrieve already known entries on startup
** delete entries etc
Insert test case here:

That still leaves dbdumper lacking but is enough for a git push
Then fix the bugs above, get sequences working etc.

17:45 mainly working now

20161130

[BUG] [FIXED] most probably the updateId used to ensure all updates are captured in write-behind is not correctly set after
startup. i imagine it reverts to 0 and needs to be set to the next correct value

[BUG][NAI] simple hazelcast example broken after the rest generation additions. [Correction -- not an issue]

[MISSING] even without file queue, dbdumper needs to be able to capture remote retrieve events and respond

[NOT OPTIMAL] if i want a hazelcast server with remote read (normal enough) i have to include a lot of guff.
split out the remote persist / retrieve into a separate generator
[ADDITIONAL] looks like generate persisters depends on generate entities and also on persistence.xml. again need to be able
to build the remote stuff without the local. the problem is that the 'direct' map stores go straight into the entities
. we could leave them out and then we'd need another geneartor for the map store factory without them in.
DO THIS NEXT. generate persisters with only remote and no entities. also make the change listener optional

[BUG] [fixed]
initial cache load causing problems with dbdumper
when the load is put into the cache the dbdumper tries to write the
entities to db again, violating primary keys

[BUG] [fixed]
has been persisted flag in impl does not appear to be used, or set properly
remove this flag. we can just check if the object is in the cache before applying an update
having a transitory field is anyway daft as it won't get into the cache or be distributed to the different nodes

[BUG BUG] [fixed. needed to use jgroups for bus]
major issues with hazelcast maps for admin messages. lots of odd behaviour and blocking
switching to broadcast messages. https://hazelcast.org/use-cases/messaging/
will have to be 2 way though and tested to death. doenst really need to be broadcast as its only between hz server
and dbdumpers

20161203

left in a mess. somehow the new admin bus has cross over between topics. messages going out on address (eg)
are coming back on head.crossover fixed but still random blocking. what is up with hazelcast

20161204 really struggling with this
topic publish blocks regardless of what i do.
maybe hazelcast can't handle publishing to topics from within a mapstore operation
not sure but its miserable. might use an alternative broadcaster to see if that fixes the issue


20161212

lots of thoughts around optimistic locking and sequences and what to support there.
optimistic locking is off in lateral so far. could enable it... might be a generator properties
could even use a blocking queue for sequs. anyway. i think the 'soft' sequences offered by hc
don't offer much. particularly if the system is restarted often

zzz . So the update id values. rubbish. or? with hc they won't sustain a restart of the system, unless
we add in some messiness. unique ids would be better. we * could * use an int/ long with optimistic locking
but i don't want to insist on that for every update.

options:
enforce optimistic locking on updates. i don't want to do that, some entities might be scratch
enough that it doesn't matter.
then we need a unique update id. can use guids but they are rather large tbh
can use the idgenerators but then we need to persist the end state or prefix with date or somesuch.
currently there is one idgen per repository. if update id stored i guess we could use that on map load.

20161213

but the reality is quite messy. the server side needs to establish the update value but the server side
doesn't really know about the repos. also it sets the map store in the config but the hazelcast instance is
needed if the map store is later to set the update id. really the map store should not know anything about hc
and the hc call map store stuff should be extended to also populate the initial ids.
Solution found using maploaderlifecyclesupport. All good.

Stuff to do now:
[a] extend retriever direct to pull in the last update id. [done]
    [a.1] will need a new jpql and query u[done]
[b] extend the admin endpoints to be able to support this remotely also
    which means a remote impl [done]
    and an endpoint [done]

[CHECK] do methods in map store need to be synchronized?

[FEATURE] next step is to add optimistic locking optionally in domain model. and then based on that we can
implement (finally) sequences. should optimistic locking be the default??
--> looking to implement now. gets into transactions though. .... need to think about that
[done]

[SEQUENCES] still on this thing. so. in order to store sequences we need a new map for storing them, and the related
entities. fine. we could then inject a hidden class into the prototypes. fine. but refactoring needed first:
[done] for now

[1] change the generators not to use the proto package except for step 1. (this would allow us to extend the proto
classes on the fly). (looks like cache and domain are ok in this regard) [done]

[LIMITATION] we'd need to check if we support subpackages within the domain prototype. i think probably not.
would need testing. perhaps will work so long as the entity names themselves are unique

[BUGS] almost there now with sequences. but. internal entities are not being created at the db. db entity names
have changed in the db, should not have _ENTITY on the end of them
almost done. I found a 'bug' in hazelcast though :( https://groups.google.com/forum/#!topic/hazelcast/Ji_g-DZwr7o
map store doesn't differentiate between put and udpate
[FIXED] worked around

Things to do stu: add to github comparison between lateral and spring boot.
different emphasis in lateral, domain is more isolated, domain objects do not need defining at the data level/entity level
unlike spring boot lateral supports write through and write behind

20170413
what to do next? could look at the search engine stuff. maybe looking at solandra would make sense. indeed could think
about schemaless storage of the data. maybe we shouldn't consider only rdbms. it should be in principle straight forward
to create new persisters which write to cassandra and perhaps don't use or need the entity classes.


20180531

Thoughts of future things to add: cassandra, solandra or similar, containerisation, what config model do we support?
Easy upgrade paths for db and hazelcast. schema changes etc.

20180909

Make the jar files executable.

20181118

Starting to get to the limits of generate-everything and lots-of-plugins. It becomes quite hard to do normal tasks
such as add in validations and so on. Everything needs to be propagated everywhere. Should we just use
generation to get things moving initially, rather than at every step of the way?

Options for validation:
add annotations to the domainLibrary classes. maybe reuse the existing entity validations
add annotations to the repository classes and stop the generation
add annotations to the entity classes and stop the generation there
add annotations, somehow, at the hazelcast level and stop the generation there
(or code).

change the domainLibrary to use classes which do the validation. would that even work given the framework?
perhaps lateral v1 has had it's day?

Experimenting with new types to encapsulate the validation.
It's a bit of a pain in the balls. IE need to swap types for the entities and then the rest loading fails.
SO maybe this isn't the way to go. maybe validations based on annotations are the way. special types are a pain
in the balls

Ok, it is possible to have a validated object type. its a little bit fiddly, you need changes to generate.properties
and changes to the class itself to support hashCode and Serialised and fromString etc. but then it does work.
not sure about the imports in the generated code though maybe that's a BUG

the other way to do this would be to use primitive types in the domain and put validation else where. but..
that's hard too.

[IMPROVEMENT] improve the generation mechanism so that the imports work for short named types used in the converter.

-> Update on the validated types. Doesn't work so well. Hazelcast search predicates etc rely on the type being mor
e basic. and that makes sense. we can't compare entries in the db if they are non primitive. So perhaps it
makes more sense to have primitive types and then validate elsewhere.

-------------------
[BUG] the put rest endpoints, if the respitory Id is a unique id, these attempt to retrieve using a string
instead of a uuid, which fails.

IE the Rest container uses a String to store the repository id and we can't just pass this into the Repository retrieve. We need to convert it back to a Unique ID first.

20190526-- Fixed
-------------------


20190527

Db dumpers. Not sure where we got to with this. It doesn't look to me today like the dbdumpers could run actve-active.
However, it would be straightforward enough to use a distributed queue with hazelcast and therefore store all changes
to the distributed store with the results being read and transferred to db then by n db dumper instances. should be fine
. we could even persist the store, but ... that's kind of the whole point of the dumper in the first place.

I'm curious about the performance of lateral. Untested as yet. Did I ever start to use Kryo?

[ISSUE] if I play with a broadcast bus setup I see that the application cannot be started until the dbdumper is running.
Is that an issue? It's blocking sending admin commands to get the latest updateId and hanging.

[BUG] seeing lots of cast problems in DB dumper when running with the example application. Something has changed since
I last run it up. (Lots of cast problems from Command to CommandResponse or vice-versa). It may just be responding to
it's own outgoing command response messages. but repeated pings to the application kills it.

20190528

Admin bus errors on db dumper. It all looks a bit of a mess in terms of how it's set up to be honest. Rationalising ...
How do we sort out the admin bus in the microservice case? That's unclear to me?

I think we wrongly differentiate between hazelcast embedded and server, muddled from the older misunderstanding still.

So in the microservice case, we use HCMapStoreRWT for example to set up the hazelcast map store. And that in turn sets
retrievers and persisters which operate on the direct local db so the admin bus is not needed at all.
Geez, supporting all this stuff really needs a tonne of test cases.

Ok, then in the broadcast bus case. Essentially each app, whether embedded or not, is the command sender side of the admin
bus. [FIX] the hazelcast server instance. So is that what we have?

Well the embedded hazelcast does create the needed queues and install a handler for the responses. I guess that kind of
makes sense. What happens on the dbdumper side? We have a chicken and egg thing. The dbdumper tries to connect to an
external hazelcast instance which doesn't exist.
    -> Really the choice of embedded or server based should be separate from the choice of command sender or command responder.
    [FIX]

Even the cache change listener is really a separate thing from an admin bus responder.

So, we have, plugins:
HazelcastEmbeddedPlugin -- the embedded hc connector
HazelcastPlugin -- the hc server connector
HazelcastCacheListenerPlugin -- really this is what listens to and perists changes heard in the cache. it is essentially
the db dumper. HazelcastCacheChangePersisterPlugin

AdminCommandSenders will block until the AdminCommandResponders respond. That's probably not idea but there u are.
Not too clear to me why this is outwith hazelcast but there were notes above about that.

AdminCommandSender and AdminCommandResponder are completely separate from the cache implementation.
We could of course have multiple active responders. We'll have to figure out how we manage that. Team Titans hehe

... except they're not. We register the admin ... gosh so darn sophisticated.

HC Cache Change Manager registers all the domain classes with admin endpoints so that when we receive an admin command
relating to each entity we can respond to it. We can even retrieve the data locally or remotely. (Circular dependencies
might ensue). Though looks like we just use local.

So the cache change manager sets up the command handler to respond to incoming admin commands.
That seems wrong, why is it in the cache change manager ...
the cache change manager really sets up the cache changer persistence.
So it's true we could separate the cache change persistence from the admin bus responder.
even though in this case they are bound to the db peristence function

Going to be fiddly due to the generation.
Steps?
(1) remove the admin command bits from the plugins
    -> kind of can't do this because the cache change plugin is bound to the admin responder for now .
(2) put them in new plugins
(3) rename the old plugins

that should be enough to get db dumper working again
then
(4) rename the hccachechangemanager and split out the admin endpoint initialisation
(4.5) remove the endpoint initialisation from the generated cache change manager
(5) update all the generators and the archetypes


Implementation notes: we might want to go even further. Basically we should be able to set an admin responder in any
app that could respond to any particular admin command. like thread dump, do this, do that. Fine.
So the admin commands we install to respond to cache initialisation, are very specific commands .

^^ need more thought. where do these commands originate ? in the remote retriever etc. so the retrieve says, the db is
remote, i'll send an admin command to retrieve the object or keys. fine. this is good, it's really ADMIN. Perhaps if
we want individual apps to have commands we can call these non-admin. but that's for later.
For now admin command responder is the one that will respond to these messages.
 (1), (2) complete.

Next task -- test the dbdumper with manually changed config and the application.
started this but null pointers on the sender side . TODO

[BUG] as part of the hazelcast embedded start up we initialise the mapstore. The mapstore makes a jgroups broadcast
call to the dbdumper to retrieve parameters such as last update Id. somehow this is causing a timeout in the hazelcast
initialisation code. So i've no idea how this worked in the past.

This means the HCMapStoreRT and other classes should not call retriever in the initialisation method. :( boohttps://uk.search.yahoo.com/yhs/search;_ylt=AwrIS.gIcu5clBUA3xN3Bwx.?p=roof+window&fr2=sb-top&hspart=ddc&hsimp=yhs-linuxmint&type=__alt__ddc_linuxmint_com`
The retriever will need to be called later in the update cycle. But of course before the application itself properly
starts. What a drag. We could move this to the repository manager perhaps. So this is growing arms and legs.

[BUG] do we have topics littered in there that we no longer use? I think they had the same problem tbh that the broadcast
bus is having now. Could be, so perhaps if we can resolve this we can revert back to using topics.
WHAT A PALAVA

Ok, i'll comment out the retrieval during the map store init and see if that fixes start up TEMPORARY
-> Yes that works fine. So I need to kick off the process after the hazelcast initialisation.

Getting too confusing for me.

Bug QUESTION. The only place the cache and the update ids can come from is the db dumper. so why not just let it populate
the cache once and for all? I suppose the answer is that different clients might need different content so why not let
them pull it as they need to. < I would hope hazelcast won't ask multiple times for the same context in the map store
impls >

It seems a pretty shit design from hazelcast that the mapstore initialisation operates in this way.

Could we just ditch the map store altogether? Just use the dumper to persist and let the app send commands to the
dumper to populate the caches in the first place?

That would be fine. The only problems here are my crazy attachment to doing everything in one framework.

Needs to be a lot simpler.
Requirements
(1) there is sometimes a need to capture all updates and reflect these to the db. even if that happens slower than real time
(2) there is sometimes a need to restore from the db. clients might not know what they need (eg all orders) but should
be able to filter out what they don't need.
(3) ideally we can support micro services as well as broadcast buses.

Think we need to do some experiments

EG 2 hc embedded apps. 1 db dumper thing. let the db dumper and apps start at different times. then load the context from
the dbdumper side. do the apps see the objects being loaded? what about the update id? that's kind of more significant
how will the apps know if the update id has been set?

In this case we'd not need an admin bus
and the mapstore would only perhaps be relevant for the micro service case.


20190530

Think need to backtrack a little and be clearer about everything.

(1) the bus and the store are quite different and will generally be handled by different apps. its only the microservice write through where they'll more likely be the same service
(2) its nice if update ids continue where they left off but not critical. hazelcast allocates 10000 block chunks so we will have gaps in the sequence numbers.
(3) the bus controller can be responsible for the loading of the bus (for now. perhaps we delegate that to another process later)
(4) the bus controller ... there's no point in it initialising the id generator. because it'll never use it and it'll lock up 10000 entries. but it could pass it out to the apps.
(5) the apps shouldn't use the repositories until the bus controller is up and ready.

Ideally we won't use an admin bus. But we might have an admin map in the hazelcast space.

OPEN ITEM: how do we handle a cache miss. Does this automatically cause a 'load' on the buscontroller side? i need to check this. Answer --> yes it does, however it's not great. If I have an app which has an embedded hc instance and i have another app which also has an embedded hc instance and the latter is a map store instance ( a db dumper ) . hc expects to be able to distribute loads across the two. even if the first app does not have a map store defined, hc will assume it's able to load some of the entries.

--> major shite really. that means the hazelcast instances need to be homogeneous. either they all load or none load. that means I can't distribute jobs to different applications. that is quite bad. I guess that's why i had such a fancy pants implementation before. all nodes delegated to one app for loading and storing.

Pretty yucky. what's the simplest solution? 
First let's see if i just switch my app to be a client and run 2 bus controllers as the backend.

-> actually seems broken. even with 2 servers and 1 client some get's are not getting processed. 

WOW BUG in hazelcast 3.6.2. Works ok in 3.11.1. Let's go back to the previous 2x embedded ...
Ops, no it is broken. It works with 1 embedded server and 1 client but not 2 embedded servers. 
That's terrible hazelcast.

BARF so. initialisation of the stores occurs when getMap is called. so that needs to be called after all the nodes are
up and running. if it happens before that point things can fail. i suppose 1 node expects another to handle the store but the other is not away. that's pretty darn flaky if you ask me. It means if the cluster goes down all the apps need to be stopped before they can be restarted. 

Ok. cluster restart looks ok. I think then just the first time we need to ensure the whole bus is up first.

Really no node should be able to call getMap until the cluster is fully up.
THIS NEEDS ENFORCING, BUT I'M NOT SURE HOW WE'D DO IT

Ok, so I think we need to choose a few operational modes to support here, we don't need to support everything.
What could those be?
(1) a microservice with embedded hazelcast that might write through to a db. might come in multiple instances
(2) a bus architecture with apps which are hazelcast clients plus a couple of bus controllers. Those controllers might write through. they might also use cassandra for speed
	what's the cassandra speed limit? looks like 10-100s of k per second with even a modest cluster.
	probably cassandra is adequate
(3) very high speed bus in which apps are hazelcast clients. we have 2 bus controllers but these do not persist. rather persistance is provided by dbdumpers with queues and loading is provided by another app. 

Think that's fine. we probably don't need (3) really. even (2) with write-behind would be pretty darn slick. 
So let's support (1) and (2) . For (1) we'll need to be able to connect to 2 hazelcast systems at the same time really. So i'd better test that. For (3) i'm not sure about loading we might need a service to provide that.

In general though it should be all very much simpler. Vastly so. I'll put service discovery into hazelcast also.

Git tagged today as v20190530. 

[TASK1] So. Let's start with the microservice. Ditch the admin bus and whittle down what's left.
We don't need any remote map store methods. 
[TASK2] make sure that the zero cache still works

[TASK3] then move on to the broadcast bus


[TASK1 -- update] first thing then. If we have a microservice. that implies embedded hazelcast. (this is what i'm calling a micoservice for these purposes, a self contained service with db). embedded tends to mean that we initiatlise the repos when the plugin is instantiated. but we can't really do that with a mapstore because the mapstore will be initialised when the first app starts and that means there might be gaps in what's stored. so this won't work. (using the mapstore to load cache miss entries won't work). 

==> need to check this with busplay 
IE 2x embedded. with mapstore loader, what happens

--> it does fail. if not all nodes are up before the first imap is called then we see failures. 

I need to report the bug. that's really poor. 
Done. What to do then? Mapstore for the microservice will only work if all instances hold off calling imap.get until all nodes are up. However we call imap get as part of the repository initialisation so it will be hard and awkward to hold off on this until all nodes are active. 

Update --> provided all the methods in MapLoader are correctly implemented this works. 

So let's continue on the assumption that (a) hazelcast will fix the bug and (b) in the short term we'll have to ensure all possible keys are loaded. 


NOTE: read-through as defined by hazelcast isn't really read-through. IE a change made to the db won't be reflected in the cache on the next read. So we have as options:

default - no mapstores
read-thru - map store with load functionality
write-thru - map store with store functionliaty and 0 delay on writes
write-behind - map store with store functionality and configured delay on writes

or better -
no load or store
load and no store == read-thru
load and store    == write-thru and write-behind

Ok, so getting there now. Will remove the topic handling also. 

[TASK] sort out property names

[BUGS] microservice is not updating update id when changes occur and it is creating garbage in the repo. --> whoops. one thing the admin bit did was to scan for the latest update id in the db and set it. we still need to do this.......
this used to happen in the cache listener / dbdumper but we're not using that now. Or rather -- this was initialised in the HCMapStorexx classes. We've since removed these calls to ensure that no app calls imap.get before all are initialised. 
Now, if we are saying all works fine provided getAllKeys does return all keys, I can reinstate this code.  [FIXED]

Ok, seems to work for now. Will be better if HC fix their bug though so we don't have to load all the keys on each start up


[BUG] PUT methods appear to allow updating of the repository id which surely should be disallowed

(1) is working fine then. Now for (2). 


20190603 PERFORMANCE WOES -----------------------------------------

Test's done today indicate hazelcast is very slow. Sending request response messages across hazelcast was seen to be pants. maybe at rates of 2.5k/second for a very very simple pojo down to 1k/ second with a hazelcast cluster. v poor i think and not what i want. deserialisation was enormously costly. not clear why. serialised objects also ludicrously huge.

so. either need to figure out good serialisation. or ... add a separate event bus.

IE we'd do ...
(1) incoming API call.
(2) create transaction object on the hc bus.
(3) send an event 'object created' state = NEW
(4) the apps would receive this and vie for control. 
(5) one would say i'll take it.
(6) then that one can load it, process it, update it
(7) then a new event would be generated -- object updated. state = COMPLETED

and so on.
It means each instance doesn't need to listen on the hazelcast bus.
That means each instance doesn't need to deserialise each object from each event.
So h/c then used for slow object transfer and fast bus for negotiations. 
all persistent stuff would need to be stored on the h/c 

ok.

ACTION -- ok, i should try topics first in hazelcast. let me try that. 
ACTION -- me to test speed of request - responses via jgroups. 
	i seem to be able to get 2k req response per second with jgroups. I'm amazed how slow it is.
	kk, up to 15k/ second if i send a byte array and skip serialization. so can i do that with hazelcast?
	could try and send a byte buffer instead. presumably that'd skip serialisation
	-> using byte buffers with hazelcast is no faster. so that seems quite poor. i think i'll leave it for now and could
	consider other ehache+jgroups based options in future. comments on the web suggest hazelcast ok, pity the reality doesn't
	appear to match
ACTION -- consider -- why bother with hazelcast? why not just persist directly to cassandra? I think that would be possibly faster anyway. 

Starting to wonder about the whole distributed processing idea tbd.
Would chain saw not be better?

Might want to investigate ignite also for use with lateral

Hazelcaset topics are very fast though. tbh i don't see the benefits of hazelcast otherwise for this type of application.
We could use topics to broadcast, have local caches, and a bus controller to ensure consistency. that'd be v fast.
So maybe jcache plus topics or ehcache plus topics.

Or ... i think we could do a nice one with each app containing a local cache (non hazelcast).
sending commands via topics to a pair of bus controllers
the buscontrollers using hazelcast so they can run active active
the relevant bus controller responding to the app
then updating the object and storing it in hazelcast (actually updating in memory then sending the event then storing)
and letting hazelcast subsequently persist it. 

That would be beautiful and superfast. So we'd have high speed broadcast bus + hazelcast for storage plus cassandra for long term storage. sex on legs. only the initial load would be involved. searches could also be run on the bus controllers.
generally fantastico. i don't know why hazelcast client doesn't do this to start with.

ref. this is me
hazel.getCluster().getLocalMember();

and this is the member a given object is stored at
hazel.getPartitionService().getPartition(key).getOwner().getAddress();

Can i use hazelcast near cache to store the caches? maybe that's clever. it'd mean we can have a lot of objects in the bus. they'd not all need to be stored in the app caches and if the app requested something that was missing we could pull it from the cluster automatically.
Maybe. Pulling entries over is slow ... or is it ? maybe it's only writing which is slow in hazelcast.
What happens with near cache if the instance is a client ? does that mean the near cache is local, i suppose it must.

kind of ironic that we're heading back towards somethink like the original admin bus 

Oh, bugs bugs. Enums causing problems with REST generation

AIM -- get the simplehazelcast example working. sort out the archetype. test microservice and simplehazelcast archetypes
AIM -- then explore cassandra as a back end

and then after than prototype a high speed bus

20190608 -- ok, updated the archetypes, simplified the code. no more admin bus. all good

Next step cassandra i think


[BUG] in microserver archtetype. pom references adminendpoints still
[FEATURE] lateral getting bloated with all possible libs that could be used in applications. i don't like that really. even if we need them to compile the code, we only need them to run the code if those particular options are used. Change the compile and build dependencies to fix this? or find another way

20190612

Part way through kundera orm 4 cassandra implementation. It's making me think about how these objects will map to cassandra entities. maybe
not so well. we'll need to do the joins and stuff ourselves. need to think about that. 

In the meantime i'm less convinced by kundera. looks undermature and i'm put off by the attitude of the responders to the bugs online. 

20190613

Ok. Well cassandra of course has no joins, but does support maps and lists. in our ORM approach in lateral we anyway break these structures by repository id. so that should be well suited to cassandra storage. IE we transform as with the RDBMS approach and store references in the entities. persist these to cassandra and so on, resolve on usage after loading. 

So we'll need a new set of persisters for cassandra, which will then call the generated transformers and then will store those objects to cassandra without using the transaction manager. this is probably a few days work, realistically. [TASK]

[TASK] -- i should also probably capture the topic based broadcast bus in a working prototype too.


20190712

[BUG] or, less than ideal anyway. RepositoryId UniqueIds are represented as strings in the REST structures. But if i add a UniqueId then that comes out
as a unique Id. So it's inconsistent. Choose one or the other. Btw, I kind of like what jersey does with the UniqueId, it's a shorter representation


20190712

New features wanted:
-- a guave cache or similar on zero cache. i'd like in memory caching for the read-only case at least
-- it'd be nice to be able to configure which imaps a client connects to. (does that make sense for hazelcast clients?)
   so that we don't have every service connecting to everything
-- it needs to be easy to configure which data isn't persisted to the db. (maybe that is in there already with the
   _sequence and others)
-- we need to be able to connect to multiple hc buses at the same time. eg: i have a config service which has it's own
   libdomain and db , but it needs to connect to a service discover hazelcast bus.


20190714

We need a mechanism in Lateral for multiple instances of a service to share the handling of work. A la team topaz as it
were in the past. I could use hazelcast with one bus per set of services but i reject that as too complex. Instead
let's investigate if we could use the (to be created) service Discovery bus for this purpose. Once we know the topology
of the service (number of instances) it's easy enough for each instance to take some of the flow. That's no bother.
What is a bother is ensuring that all tasks are done once only when new service instances join the cluster or (harder)
when an instance leaves the cluster.

Let's investigate.

We're going to need a simple way for an application to be part of multiple elastic cachces / domains in any case.
This in the end looks like meaning we'll need to support multiple repository managers at the same time.

Which doesn't look impossible but tricky. We'll need a new FactoryContract implementation which will serve the classes
from multiple domains.
Then ...
We'll need to initialise multiple HCRepositoryManagerImpl.

I wonder what the easiest way to do this is. We almost can do it out of the box.
Which is to say ... if the Application DI knew about contexts then it could vary it's behaviour depending on whether
we were in context 1 or context 2. And we could define those contexts based on the libdomain package name.

(It is supposed to be CDI after all)

Very curious. If we can get it for almost free then so much the better.
Probably very hard though, we'd need to proceed with utmost caution.
We need a way to establish which context we are in. That could be down to the package name of the calling class.
Then depending on the context we would vary the ApplicationDI inject responses.

Hmm, though there are cases where this would not work. Eg Factory, it has 1 default implemention but we'd need two.

It would almost be easier if we could run 2 apps. Or, is it just the singletons which are getting in the way here?
I don't want to open pandora's box here. My thoughts lead me to byte code injection and context boundaries and blurg.
I still want to keep it simple.

Perhaps there are just a few singleton classes we need to bridge across the various contexts?

Might only be factory that needs bridging. Not finding other static final vars.

For factory we'd need an overarching implementation. (Which might mean we want no name conflicts across domains)
Lateral plugins would need the context set before they were instantiated.

Seems a bit too ropey to me. And yet it shouldn't be. Really. We should be able to separate these in principle.
AnnotationScanner and LateralPluginManager are also singletons. Plus lateral itself of course.

Might get really really funky.
eg ApplictionDI being a facade which could have multiple implementation -- one to use in single context and one to use
in multiple context. That means we might need 1xapplication di per domain plus 1 for the common framework.

Quite a pity. In topaz connecting to multiple busses was a cinch.

It's just the use of statics here that is causing the problems.

Even if we do set it up so we can essentially run multiple laterals in one JVM we'll need, what?
libdomain1, libdomain2 etc.

it's almost like creating our own class loader

I suppose really we should ditch all singletons, all statics, and use n x appliction di to instantiate everything.
Lateral Core by and large doesn't use inject. Just in the plugins and for the factory method.

** how much do users use inject ? if it only occurs in generated code, it'd be easy enough to add parameters to
Not much I think. Though there's nothing stopping them doing so.

We might just need 1 or 2 very clear helper statics
Factory
Repository
Lateral

If multiple contexts are defined you'd have to either specify the context on inject, or, specify it programmatically.
I think i'd prefer not to have bridging for factory tbh.

Factory     factory1 = inject( Factory.class, Domain1Context );
Repository  repository1 = inject( Repository.class, Domain1Context );

MyDomain1Object o1 = factory1.create( MyDomain1Object.class );

I think that's better, clearer.
So we'd only need inject as a helper really.

Might be possible but it's stretching it a bit.

I'm assuming some basic stuff here, like we -can- connect to multiple hazelcast clusters from one application.
Not sure if that's true or not.

If I changed Factory and Respository like above, could I not just change all the generated code to specify the context
explicitly? Maybe we could get all the generated code to inject explicitly, including the plugins.
Also , for the server side, perhaps the maven generators can check for the existence of multiple ...
ah tricky with persistence. They'd all be stored in the same db anyway. blurg.

need to think the use cases:
(1) an app with its own domain , own db. which also uses (eg service discovery) on another hazelcast bus
(2) a server with 2 domains on the same hazelcast bus (eg transactional oltp and service discovery). both might be
stored in the same db. maybe. or in multiple dbs. Maybe that's fine, we'd just need multiple persistence units in the
persistence.xml file. we'd probably need multiple generate-xxx.property files

So. possible i think. Not too messy. But a lot of work.

And all that before we can start to think about service discovery.
Maybe-i-am-trying-to-solve-the-wrong-problem

We could at least say 1 server (cluster) per domain to begin with.

20190915

Well, to roll it back a bit. I can add service discovery as internal classes to lateral within the users domain.
That would allow multiple instances to co-ordinate their activities. I don't need a re-write of lateral for that.
Step 1 would be allowing applications to not connect to all the maps on the bus.
Will be quite tricky to co-ordinate well across nodes if there are nodes failing etc.
team topaz!


20190730

Big changes ongoing to support multiple domains in lateral. This is meaning -- new DependencyInjection, removal of a
lot of the singleton classes in favour of instances accessible through a Lateral singleton class. (ie just 1 place we
have singletons).

Plugins need reworking now. Some will be instantiated per context and some can be across all contexts.
I'll make this a lateralplugin parameter

My note:

Update grizzly to operate cross-context
Update all the generators to specify the context and create the context class.
Update the entity generation to specify different persistence units per context, or add to generate.properties.

Convert factory, repository etc to be non - singletons and put helpers perhaps

20190812

Repository and Factory then.
Repository references Factory , well. hmm. Factory get Repository for class could span multiple domains. Fully qualified
classes are unlikely to conflict across domains. Though the current factory class is a stupid wrapper and anyway defers
to a domain specific default factory implementation.

So. we could have a domain specific repository which references a domain specific factor

RepositoryDomainA { FactoryIF f = inject(FactoryIF.class, DomainA.class); f.getRepositoryForClas(xxx) }

which is basically fine. We'll need to generate the repository and factory per domain. But we generate a factory class
per domain already so that's fine.

The only challenge then is the convenience statics and maybe they're not really such a big deal anyway.

Copying the originals to doc/lateral-bu for now.

Renaming Factory Contract ... hang on ...yes. Renaming Factory Contract to Factory and this becomes the interface.

Done. Creating a repository interface... ok doke.

Can we use the existing DefaultFactory as the Factory implementation? --> looks pretty much like it. Updating
generator for DefaultFactory to include the context.

kk, repo is slightly confusing. The class-specific generated repos implement 'crudrepository' interface.
Thats a typed interface and there are many of them.

Repository and it's implementation are slightly different and control access to all of these. Repository uses the
CRUDRepository interface but doesn't need to implement it. We'll probably need better naming at some point.

Starting to create the DefaultRepository and generator ...
[TODO] kk, template created. Now need the generator ... tomorrow. [DONE?]
[TODO] add generator to create the LateralDIContext class, like in <buscontroller>

Then should be ready to test on an example domain.

20190817 [TROUBLE]

circular dependency now in first run of new domain. ie new lateral v2 core with simple microservice libdomain.
HCRepoManager needs to register the repository implemenetations.
But in order to create each it needs to inject Factory implementation
and DefaultFactoryImplementation in turn needs to inject all the Repos.

So, it dunt work. I think the HCRepoManager needs to be able to create the repo impls without triggering the injects.
OR factory shouldn't be bound to the repositories.

Perhaps HCRepoManager can register the class instead of the implementation. Hmm
--> no, can't do that as it needs constructor parameters. Sheet.

So, I could move the getRepositoryForClass into DefaultRepository from DefaultFactory. that would make sense.

But still HCRepositoryManager would need to register the repository implementations
and each impl needs to inject the Repository which in turn has references to all the repositories.
In some ways the only problem is that we do the injection in the constructor of (as now) DefaultFactoryImpl
instead of later.

All of the repository implementations come from HCRepositoryManager, so this class could trigger the default
factory implementation to initiatlise once all the repositories are created. That would work fine.

steps [1] move get repo for class from Factory to Repository. (think about renaming repository)
[2] move repo registration from constructor to initialisation method
[3] call initiatlisation method in HCRepositoryManager.

[1] and [2] are DONE
[3] also, though modified.

HazelcastPlugin and ZerocachePlugin use inject without the context. fix this.
ZCRepositoryManager appears to have disappeared. And the impls too. ?

20190823

Need to repeat the exercise now with the entities and application level stuff.
Obvious things that need to change:
(*) the transactionmanager impl will need to be generated per-context and the persistent context name will need to be
provided and made overridable in configuration
(*) we'll need to be able to specify multiple domain.generated (or similar) in the generate.properties. It will need to
be possible to generate entities across several domains if needed.That might get messy. could use separate files.
(*) nice to have -- the rest and grizzly will support cross domain working if needed
(*) all the generated files will need to be updated to include the context of course.
(*) persistence.xml might need to support multiple PUs also
(*) answer the question -- what if i want to use a single hazelcast cache for multiple domains ? [DONE] -- looks perfectly
possible to start 2x hazelcast embedded instances in the same application on the same ports.

Steps
(1) build out the multiple run generation. parse the single or multiple generate-.properties. barf if the contexts are
not defined in these. Update all the generators to pass in the context.[IN PROGRESS . generateEntities all updated]
(2) create a transaction manager class per context.

Probably sort out the persistence.xml files after that.




20190901

All building ok now. I need a lot of examples to test with:
(1) multiple domains, with 1 hazelcast cache, 1 db, rest and grizzly which span both domains
(2) multiple domains with multiple hazelcast and multiple dbs.

etc

Oh first -- TASK == sort out the transaction manager. we'll need one per context now

20190902 --> ok, done


20190912

Spotting a bug in the storage and setting of sequence ids.
Kind of a bummer. Really the factory needs to use the repository when doing a create in order to set any sequence ids
properly. otherwise, uninitialised ids can sometimes not be persisted correctly to the db.
I don't want that link between factor and repository but i think its still what needs to be done. Perhaps at some point
we'll just combine the factory and repository and that'll be that.

For now i need to update the generators and move code around a bit.

the problem occurred when i had a class with a sequence id and then i persisted it as part of a larger object.
because it was persisted after the parent class, the parent class reference to the class (via a list) just contained
a null id. so we'd either need to set the ids before persisting or we'd need to sort the persisting classes into
order and persist them in that order to ensure ids were always set before references were stored.

It means moving some code out of the hazelcast cache generation and into the generic domain stuff which i'm slightly
uneasy about. Though the update sequence code is identical in the different cache impls anyway.

20190916

Looking good. The broadcastbus archetype needs updating but the simple microservice is fine.

20200214

Need to remind myself where we got to with all this. I'm thinking more i need to re-review Kubernetes, do we really need
to support multiple domains in a single application or are there other ways around this?

